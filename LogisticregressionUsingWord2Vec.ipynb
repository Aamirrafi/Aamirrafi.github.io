{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LogisticregressionUsingWord2Vec",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOHhxVy5+08gqmo/Yv10YcT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aamirrafi/Aamirrafi.github.io/blob/master/LogisticregressionUsingWord2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQB_oib5zSej",
        "outputId": "76f9b933-ed39-431b-a79b-d6f304018d35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        }
      },
      "source": [
        "!pip install -Uqq fastbook\n",
        "import fastbook\n",
        "fastbook.setup_book()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 727kB 5.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 28.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 9.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 194kB 42.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 7.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 12.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 40kB 6.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 7.9MB/s \n",
            "\u001b[K\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d4b7c5385581>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install -Uqq fastbook'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfastbook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfastbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_book\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fastbook'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqVuWQoN0J3k"
      },
      "source": [
        "from fastbook import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmLA77rh0KYp",
        "outputId": "58e8a2b9-267c-44dd-a8de-eda53d23aaad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install --upgrade gensim\n",
        "!pip install nltk\n",
        "!pip install tqdm\n",
        "!pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.1-cp27-none-linux_x86_64.whl\n",
        "!pip install keras\n",
        "!pip install bokeh\n",
        "!pip install ipdb"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: gensim in /usr/local/lib/python3.6/dist-packages (3.8.3)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (3.0.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "\u001b[31mERROR: tensorflow-1.0.1-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.4.3)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras) (1.15.0)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.6/dist-packages (2.1.1)\n",
            "Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.6/dist-packages (from bokeh) (2.11.2)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from bokeh) (3.13)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.6/dist-packages (from bokeh) (5.1.1)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.6/dist-packages (from bokeh) (20.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.6/dist-packages (from bokeh) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from bokeh) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from bokeh) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.0 in /usr/local/lib/python3.6/dist-packages (from bokeh) (7.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7->bokeh) (1.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging>=16.8->bokeh) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=16.8->bokeh) (2.4.7)\n",
            "Requirement already satisfied: ipdb in /usr/local/lib/python3.6/dist-packages (0.13.4)\n",
            "Requirement already satisfied: ipython>=5.1.0; python_version >= \"3.4\" in /usr/local/lib/python3.6/dist-packages (from ipdb) (5.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from ipdb) (50.3.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (4.3.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.6.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython>=5.1.0; python_version >= \"3.4\"->ipdb) (0.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uky-5HUMEiw"
      },
      "source": [
        "import pandas as pd # provide sql-like data manipulation tools. very handy.\n",
        "pd.options.mode.chained_assignment = None\n",
        "import numpy as np # high dimensional vector computing library.\n",
        "from copy import deepcopy\n",
        "from string import punctuation\n",
        "from random import shuffle\n",
        "\n",
        "import gensim\n",
        "from gensim.models.word2vec import Word2Vec # the word2vec model gensim class\n",
        "LabeledSentence = gensim.models.doc2vec.LabeledSentence # we'll talk about this down below\n",
        "\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"progress-bar\")\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\n",
        "tokenizer = TweetTokenizer()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lE3NYpqMulW",
        "outputId": "b151aaec-8065-4cba-caa5-a8e80f81e56b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import os;\n",
        "cwd = os.getcwd()\n",
        "cwd"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQTsEdWHP4xg",
        "outputId": "7cbcb0ac-0f2b-4ce5-f351-f3c722d1386d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "data = pd.read_csv('/content/Ethos_Dataset_Binary.csv', sep='_')\n",
        "data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment;isHate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>You should know women's sports are a joke;1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>You look like Sloth with deeper Down’s syndrom...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>You look like Russian and speak like Indian. B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Women deserve to be abused, I guess.;1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Women are made for making babies and cooking d...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      comment;isHate\n",
              "0      You should know women's sports are a joke;1.0\n",
              "1  You look like Sloth with deeper Down’s syndrom...\n",
              "2  You look like Russian and speak like Indian. B...\n",
              "3           Women deserve to be abused, I guess.;1.0\n",
              "4  Women are made for making babies and cooking d..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9Gebk4cZrZq",
        "outputId": "9619b57b-e55b-4fba-985f-fbf9c3c96c58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "texts = []\n",
        "labels = []\n",
        "data.shape\n",
        "a = data[\"comment;isHate\"][0]\n",
        "a\n",
        "for _ in range(data.shape[0]):\n",
        "    a = data[\"comment;isHate\"][_].split(';')\n",
        "    texts.append(a[0])\n",
        "    labels.append(a[1])\n",
        "df = pd.DataFrame({\"Sentiment\":texts, \"Label\":labels})\n",
        "df.head()\n",
        "# df.shape\n",
        "# df['Sentiment'][1]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>You should know women's sports are a joke</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>You look like Sloth with deeper Down’s syndrome</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>You look like Russian and speak like Indian. B...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Women deserve to be abused, I guess.</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Women are made for making babies and cooking d...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Sentiment Label\n",
              "0          You should know women's sports are a joke   1.0\n",
              "1    You look like Sloth with deeper Down’s syndrome   1.0\n",
              "2  You look like Russian and speak like Indian. B...   1.0\n",
              "3               Women deserve to be abused, I guess.   1.0\n",
              "4  Women are made for making babies and cooking d...   1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bor5wf_DzX7u",
        "outputId": "53bf02d0-a776-4c97-b7a5-f603b7fc90c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "import ipdb as ipdb\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sSJN6Mwiz-r"
      },
      "source": [
        ""
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsqEApZaof7z",
        "outputId": "6496a7d4-f2c8-4820-fd5d-1fe3e82d2aa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# print(df[\"Sentiment\"][0])\n",
        "\n",
        "tokenList = []\n",
        "reviews = []\n",
        "# from nltk.tokenize import word_tokenize\n",
        "for i in range(df.shape[0]):\n",
        "  reviews.append(df[\"Sentiment\"][i].lower().split())\n",
        "reviews[:3]\n",
        "# word = []\n",
        "# remove_stopwords = True\n",
        "# for review in reviews:\n",
        "#   # print(review)\n",
        "#   if (len(review) > 0):\n",
        "#     # words = df['Sentiment'][0] \n",
        "#     if remove_stopwords:\n",
        "#       stops = set(stopwords.words(\"english\"))\n",
        "#       ipdb.set_trace()\n",
        "#       for w in review:\n",
        "#         if w in stops:\n",
        "#           pass\n",
        "#         else:\n",
        "#           word.append(w)\n",
        "#     # review = word\n",
        "#     # word = []\n",
        "# review[:2]\n",
        "# # tokenList[:10]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # raw_sentences = tokenizer.tokenize(df.strip())\n",
        "    # df['Sentiment'][i] = word_tokenize(df['Sentiment'][i])\n",
        "# print(len(reviews))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['you', 'should', 'know', \"women's\", 'sports', 'are', 'a', 'joke'],\n",
              " ['you', 'look', 'like', 'sloth', 'with', 'deeper', 'down’s', 'syndrome'],\n",
              " ['you',\n",
              "  'look',\n",
              "  'like',\n",
              "  'russian',\n",
              "  'and',\n",
              "  'speak',\n",
              "  'like',\n",
              "  'indian.',\n",
              "  'both',\n",
              "  'are',\n",
              "  'disgusting',\n",
              "  'go',\n",
              "  'kill',\n",
              "  'yourself']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y44ZA5y5uGMF",
        "outputId": "2b096216-9ed4-4d10-a1ac-e2cf9d690bf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# print(df[\"Sentiment\"][0].split())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['You', 'should', 'know', \"women's\", 'sports', 'are', 'a', 'joke']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8yHvF86o15D",
        "outputId": "10068df3-6f5a-4647-d8fc-c8d7d1a985ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[You, should, know, women, 's, sports, are, a,...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[You, look, like, Sloth, with, deeper, Down, ’...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[You, look, like, Russian, and, speak, like, I...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Women, deserve, to, be, abused, ,, I, guess, .]</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Women, are, made, for, making, babies, and, c...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Sentiment Label\n",
              "0  [You, should, know, women, 's, sports, are, a,...   1.0\n",
              "1  [You, look, like, Sloth, with, deeper, Down, ’...   1.0\n",
              "2  [You, look, like, Russian, and, speak, like, I...   1.0\n",
              "3   [Women, deserve, to, be, abused, ,, I, guess, .]   1.0\n",
              "4  [Women, are, made, for, making, babies, and, c...   1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z1CyDQiixLv",
        "outputId": "9c39fb7c-749d-49af-fb61-841dfa57504f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "stop_words= True\n",
        "word = []\n",
        "words = df['Sentiment'][0]\n",
        "# if remove_stopwords:\n",
        "#   stops = set(stopwords.words(\"english\"))\n",
        "#   for w in words:\n",
        "#     if not w in stops:\n",
        "#       word.append(w)\n",
        "words"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"You should know women's sports are a joke\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW0aCB4alxyQ",
        "outputId": "957aceb5-c2f0-423e-88df-2716943b7537",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "tokenList[2]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-8a2034bab2d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKgtAcCvXTC2",
        "outputId": "c3b0915b-4db7-43c1-b715-f4bd374e179f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        }
      },
      "source": [
        "tweet_w2v = Word2Vec(size=50, min_count=10)\n",
        "tweet_w2v.build_vocab([x.words for x in tqdm(x_train)])\n",
        "tweet_w2v.train([x.words for x in tqdm(x_train)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 798/798 [00:00<00:00, 448967.75it/s]\n",
            "\n",
            "100%|██████████| 798/798 [00:00<00:00, 880803.84it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-142-d921e453183a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtweet_w2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtweet_w2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtweet_w2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             total_words=total_words, **kwargs)\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[0;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtotal_examples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             raise ValueError(\n\u001b[0;32m-> 1186\u001b[0;31m                 \u001b[0;34m\"You must specify either total_examples or total_words, for proper job parameters updation\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m                 \u001b[0;34m\"and progress calculations. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m                 \u001b[0;34m\"The usual value is total_examples=model.corpus_count.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: You must specify either total_examples or total_words, for proper job parameters updationand progress calculations. The usual value is total_examples=model.corpus_count."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBFNJdXetpy2",
        "outputId": "f38a432b-0308-4965-c67c-37a39766cfc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/Ethos_Dataset_Binary.csv', sep ='_')\n",
        "df.head()\n",
        "# df.shape\n",
        "# type(df['Label'][0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment;isHate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>You should know women's sports are a joke;1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>You look like Sloth with deeper Down’s syndrom...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>You look like Russian and speak like Indian. B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Women deserve to be abused, I guess.;1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Women are made for making babies and cooking d...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      comment;isHate\n",
              "0      You should know women's sports are a joke;1.0\n",
              "1  You look like Sloth with deeper Down’s syndrom...\n",
              "2  You look like Russian and speak like Indian. B...\n",
              "3           Women deserve to be abused, I guess.;1.0\n",
              "4  Women are made for making babies and cooking d..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGUkVThZEOy_"
      },
      "source": [
        "texts = []\n",
        "labels = []\n",
        "# data.shape\n",
        "a = df[\"comment;isHate\"][0]\n",
        "# a\n",
        "for _ in range(df.shape[0]):\n",
        "    a = df[\"comment;isHate\"][_].split(';')\n",
        "    # texts.append(a[0])\n",
        "    a0 = ''\n",
        "    for i in a[:-1]:\n",
        "      a0 = a0 + ' ' + i\n",
        "    texts.append(a0)\n",
        "    labels.append(a[-1])\n",
        "df = pd.DataFrame({\"Sentiment\":texts, \"Label\":labels})\n",
        "# df['Label'] = pd.to_numeric(df['Label'])\n",
        "# df['Label'] = (df['Label'] < 0.5).astype(int)\n",
        "# df['Label'] = (df['Label'] >= 0.5).astype(int)\n",
        "# df\n",
        "\n",
        "# df['Label']=[df['Label'] < 0.5] \n",
        "# df['Label'][df['Label'] > 0] = 1"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DX2sbQaE-6T",
        "outputId": "a92820d4-b132-4278-92ab-512e8e7da34f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df['Label'][195:200]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "195    0.8333333333333334\n",
              "196    0.8333333333333334\n",
              "197    0.8333333333333334\n",
              "198    0.8333333333333334\n",
              "199    0.8333333333333334\n",
              "Name: Label, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWUlBq8kGhpg",
        "outputId": "19bbfa28-6314-42c2-e73a-d93c414d9f2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "# Removing the stop words\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "print(remove_stopwords(\"Restaurant had a really good service!!\"))\n",
        "print(remove_stopwords(\"I did not like the food!!\"))\n",
        "print(remove_stopwords(\"This product is not good!!\"))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Restaurant good service!!\n",
            "I like food!!\n",
            "This product good!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ8CbO3GIeYD",
        "outputId": "5c7741a1-f70d-466d-b712-8fa090925d06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "from gensim.utils import simple_preprocess\n",
        "# Tokenize the text column to get the new column 'tokenized_text'\n",
        "df['Sentiment'] = [remove_stopwords(line) for line in df['Sentiment']]\n",
        "df['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in df['Sentiment']] \n",
        "# print(df['tokenized_text'].head(10))\n",
        "# print(df.head())\n",
        "len(df)\n",
        "df.shape\n",
        "df.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Label</th>\n",
              "      <th>tokenized_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>You know women's sports joke</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[you, know, women, sports, joke]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>You look like Sloth deeper Down’s syndrome</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[you, look, like, sloth, deeper, down, syndrome]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>You look like Russian speak like Indian. Both ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[you, look, like, russian, speak, like, indian...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Women deserve abused, I guess.</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[women, deserve, abused, guess]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Women making babies cooking dinner else!!!</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[women, making, babies, cooking, dinner, else]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Sentiment  ...                                     tokenized_text\n",
              "0                       You know women's sports joke  ...                   [you, know, women, sports, joke]\n",
              "1         You look like Sloth deeper Down’s syndrome  ...   [you, look, like, sloth, deeper, down, syndrome]\n",
              "2  You look like Russian speak like Indian. Both ...  ...  [you, look, like, russian, speak, like, indian...\n",
              "3                     Women deserve abused, I guess.  ...                    [women, deserve, abused, guess]\n",
              "4         Women making babies cooking dinner else!!!  ...     [women, making, babies, cooking, dinner, else]\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltxJ4bDQI53b",
        "outputId": "1a01d9b3-9d58-4d5e-ed11-9b495b00c3d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Label</th>\n",
              "      <th>tokenized_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>You know women's sports joke</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[you, know, women, sports, joke]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>You look like Sloth deeper Down’s syndrome</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[you, look, like, sloth, deeper, down, syndrome]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>You look like Russian speak like Indian. Both ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[you, look, like, russian, speak, like, indian...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Women deserve abused, I guess.</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[women, deserve, abused, guess]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Women making babies cooking dinner else!!!</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[women, making, babies, cooking, dinner, else]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Sentiment  ...                                     tokenized_text\n",
              "0                       You know women's sports joke  ...                   [you, know, women, sports, joke]\n",
              "1         You look like Sloth deeper Down’s syndrome  ...   [you, look, like, sloth, deeper, down, syndrome]\n",
              "2  You look like Russian speak like Indian. Both ...  ...  [you, look, like, russian, speak, like, indian...\n",
              "3                     Women deserve abused, I guess.  ...                    [women, deserve, abused, guess]\n",
              "4         Women making babies cooking dinner else!!!  ...     [women, making, babies, cooking, dinner, else]\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eVKLTRAWPzX",
        "outputId": "be7d22da-67c8-48fe-bef5-a58e135c9f6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "from gensim.parsing.porter import PorterStemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "# Get the stemmed_tokens\n",
        "df['stemmed_tokens'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in df['tokenized_text'] ]\n",
        "df['stemmed_tokens'].head(10)\n",
        "df.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Label</th>\n",
              "      <th>tokenized_text</th>\n",
              "      <th>stemmed_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>You know women's sports joke</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[you, know, women, sports, joke]</td>\n",
              "      <td>[you, know, women, sport, joke]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>You look like Sloth deeper Down’s syndrome</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[you, look, like, sloth, deeper, down, syndrome]</td>\n",
              "      <td>[you, look, like, sloth, deeper, down, syndrom]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>You look like Russian speak like Indian. Both ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[you, look, like, russian, speak, like, indian...</td>\n",
              "      <td>[you, look, like, russian, speak, like, indian...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Women deserve abused, I guess.</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[women, deserve, abused, guess]</td>\n",
              "      <td>[women, deserv, abus, guess]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Women making babies cooking dinner else!!!</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[women, making, babies, cooking, dinner, else]</td>\n",
              "      <td>[women, make, babi, cook, dinner, els]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Sentiment  ...                                     stemmed_tokens\n",
              "0                       You know women's sports joke  ...                    [you, know, women, sport, joke]\n",
              "1         You look like Sloth deeper Down’s syndrome  ...    [you, look, like, sloth, deeper, down, syndrom]\n",
              "2  You look like Russian speak like Indian. Both ...  ...  [you, look, like, russian, speak, like, indian...\n",
              "3                     Women deserve abused, I guess.  ...                       [women, deserv, abus, guess]\n",
              "4         Women making babies cooking dinner else!!!  ...             [women, make, babi, cook, dinner, els]\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ8A6ddpWSnR"
      },
      "source": [
        "df['Label'] = df['Label'].astype(float)\n",
        "df['Label'][df['Label']>=0.5]=1\n",
        "df['Label'][df['Label']< 1]=0\n",
        "df['Label'] = df['Label'].astype(int)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92Z_92eROlg2",
        "outputId": "52de7daf-9602-4d83-fd85-2ca2e02d34f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df.shape\n",
        "df[417:422]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Label</th>\n",
              "      <th>tokenized_text</th>\n",
              "      <th>stemmed_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>417</th>\n",
              "      <td>France let refugees happened fucking animals</td>\n",
              "      <td>1</td>\n",
              "      <td>[france, let, refugees, happened, fucking, ani...</td>\n",
              "      <td>[franc, let, refuge, happen, fuck, anim]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>418</th>\n",
              "      <td>Every female-dominated job: low skill minimal ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[every, female, dominated, job, low, skill, mi...</td>\n",
              "      <td>[everi, femal, domin, job, low, skill, minim, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>419</th>\n",
              "      <td>Easter silly holiday existed. It stopped. Sham...</td>\n",
              "      <td>1</td>\n",
              "      <td>[easter, silly, holiday, existed, it, stopped,...</td>\n",
              "      <td>[easter, silli, holidai, exist, it, stop, sham...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420</th>\n",
              "      <td>Do world favor pour bleach glass</td>\n",
              "      <td>1</td>\n",
              "      <td>[do, world, favor, pour, bleach, glass]</td>\n",
              "      <td>[do, world, favor, pour, bleach, glass]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>421</th>\n",
              "      <td>Disgusting sick queer kill</td>\n",
              "      <td>1</td>\n",
              "      <td>[disgusting, sick, queer, kill]</td>\n",
              "      <td>[disgust, sick, queer, kill]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Sentiment  ...                                     stemmed_tokens\n",
              "417       France let refugees happened fucking animals  ...           [franc, let, refuge, happen, fuck, anim]\n",
              "418  Every female-dominated job: low skill minimal ...  ...  [everi, femal, domin, job, low, skill, minim, ...\n",
              "419  Easter silly holiday existed. It stopped. Sham...  ...  [easter, silli, holidai, exist, it, stop, sham...\n",
              "420                   Do world favor pour bleach glass  ...            [do, world, favor, pour, bleach, glass]\n",
              "421                         Disgusting sick queer kill  ...                       [disgust, sick, queer, kill]\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nu2q1NSJuAi",
        "outputId": "ab712494-b48a-4c19-e23c-3a54f9add153",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Train Test Split Function\n",
        "x_train, x_test, y_train, y_test = train_test_split(df[['stemmed_tokens']],df[['Label']], test_size = 0.2)\n",
        "\n",
        "# print(\"Value counts for Train sentiments\")\n",
        "# print(x_train[:10])\n",
        "# # print(y_train.value_counts())\n",
        "# print(\"Value counts for Test sentiments\")\n",
        "# # print(y_test.value_counts())\n",
        "# print(type(x_train))\n",
        "# print(type(y_train))\n",
        "# X_train = x_train.reset_index()\n",
        "# X_test = x_test.reset_index()\n",
        "# Y_train = y_train.to_frame()\n",
        "# Y_train = y_train.reset_index()\n",
        "# Y_test = y.test.to_frame()\n",
        "# Y_test = y_test.reset_index()\n",
        "# print(x_train.head())\n",
        "x_train.head()\n",
        "y_train.head()\n",
        "\n",
        "y_test = y_test.to_numpy()\n",
        "y_test = y_test.ravel()\n",
        "x_train.shape , y_train.shape,x_test.shape,y_test.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((798, 1), (798, 1), (200, 1), (200,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffZHWhhYJ2P4",
        "outputId": "967fb2dc-8cfd-42a4-a1e6-883b7f5e9150",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "from gensim.models import Word2Vec\n",
        "import time\n",
        "# Skip-gram model (sg = 1)\n",
        "size = 1000\n",
        "window = 5\n",
        "min_count = 1\n",
        "workers = 3\n",
        "sg = 1\n",
        "\n",
        "word2vec_model_file = 'word2vec_' + str(size) + '.model'\n",
        "start_time = time.time()\n",
        "stemmed_tokens = pd.Series(df['stemmed_tokens']).values\n",
        "print(type(stemmed_tokens))\n",
        "# Train the Word2Vec Model\n",
        "w2v_model = Word2Vec(stemmed_tokens, min_count = min_count, size = size, workers = workers, window = window, sg = sg)\n",
        "print(\"Time taken to train word2vec model: \" + str(time.time() - start_time))\n",
        "w2v_model.save(word2vec_model_file)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "Time taken to train word2vec model: 1.3799009323120117\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQdtds1Q_VZ8",
        "outputId": "1635a87c-37f7-48dd-ff01-0e7ff9499537",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import gensim.downloader\n",
        "print(list(gensim.downloader.info()['models'].keys()))\n",
        "word2vec_google = gensim.downloader.load('word2vec-google-news-300')\n",
        "# glove_vectors = gensim.downloader.load('glove-twitter-25')\n",
        "# w2v_model.most_similar('bad')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koVBXIUhLnaa",
        "outputId": "756af270-db72-42b2-b959-a26c93c086a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word2vec_google.most_similar('bad')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('good', 0.7190051078796387),\n",
              " ('terrible', 0.6828612089157104),\n",
              " ('horrible', 0.6702598333358765),\n",
              " ('Bad', 0.6698919534683228),\n",
              " ('lousy', 0.6647640466690063),\n",
              " ('crummy', 0.5677819848060608),\n",
              " ('horrid', 0.5651682615280151),\n",
              " ('awful', 0.5527253150939941),\n",
              " ('dreadful', 0.5526429414749146),\n",
              " ('horrendous', 0.5445997714996338)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVxO-Rg9XIuq",
        "outputId": "3b6376d4-864d-4d79-a886-befa79f695f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "# Load the model from the model file\n",
        "sg_w2v_model = Word2Vec.load(word2vec_model_file)\n",
        "# Unique ID of the word\n",
        "print(\"Index of the word 'action':\")\n",
        "print(sg_w2v_model.wv.vocab[\"bad\"].index)\n",
        "# Total number of the words \n",
        "print(len(sg_w2v_model.wv.vocab))\n",
        "# Print the size of the word2vec vector for one word\n",
        "print(\"Length of the vector generated for a word\")\n",
        "print(len(sg_w2v_model['bad']))\n",
        "# Get the mean for the vectors for an example review\n",
        "print(\"Print the length after taking average of all word vectors in a sentence:\")\n",
        "print(np.mean([sg_w2v_model[token] for token in df['stemmed_tokens'][0]], axis=0))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index of the word 'action':\n",
            "146\n",
            "2848\n",
            "Length of the vector generated for a word\n",
            "1000\n",
            "Print the length after taking average of all word vectors in a sentence:\n",
            "[ 2.95611359e-02 -1.68830659e-02  1.89593844e-02  3.60552147e-02\n",
            "  3.04069761e-02  8.83459020e-03  2.73585506e-03 -1.51904374e-02\n",
            "  2.78498530e-02  1.50953829e-02  1.34625332e-02  7.97531195e-03\n",
            " -2.30798572e-02  1.06145348e-02  2.45911386e-02 -4.62506013e-03\n",
            " -8.86661001e-03  2.55360063e-02 -4.03433945e-03  2.16964330e-03\n",
            " -7.28171784e-03  3.05106468e-03 -7.50747835e-03  1.57062504e-02\n",
            "  5.85178053e-03 -1.00259762e-02  1.91764683e-02  7.55373109e-03\n",
            "  5.08240145e-03  7.51805212e-03  1.30863336e-03 -1.51058077e-03\n",
            "  1.36013683e-02  1.35990430e-03  2.50532432e-03  6.25121221e-03\n",
            " -5.60130458e-03 -1.19573567e-02 -1.17274141e-02  3.43374535e-02\n",
            " -4.98635601e-03 -1.56415589e-02  6.61883270e-03  4.82487958e-03\n",
            "  8.48158263e-03  1.29515063e-02  1.04118641e-02 -2.52473466e-02\n",
            "  9.28310491e-03  2.19841544e-02  1.24139385e-02  1.93488728e-02\n",
            "  1.64821297e-02 -4.14317939e-03  1.81040838e-02 -1.99521426e-02\n",
            "  2.64521576e-02 -3.23075466e-02 -4.94497037e-03  1.03312340e-02\n",
            "  1.36064021e-02 -2.87478901e-02  1.02558387e-02  1.60855148e-02\n",
            " -1.35123599e-02 -3.82779911e-03 -1.70079097e-02 -1.52011644e-02\n",
            "  6.03882270e-03 -2.14699097e-02  1.02586448e-02 -5.36912167e-03\n",
            " -1.35587808e-02 -4.75027179e-03  8.74551758e-03  3.13750729e-02\n",
            "  1.60930008e-02  3.72491707e-03  1.95060521e-02 -1.44447265e-02\n",
            "  1.50687818e-03 -4.12395187e-02  1.51148085e-02 -1.62840039e-02\n",
            " -1.14313234e-02  1.38646886e-02 -2.53782719e-02 -1.77512188e-02\n",
            " -3.29689425e-03 -2.64614169e-02  3.43145477e-03  9.62928124e-03\n",
            " -5.05671185e-03  1.54726044e-03 -1.10619748e-02  5.80792036e-03\n",
            " -1.22979917e-02 -6.54355343e-03 -3.10371369e-02  4.55553690e-03\n",
            " -1.14630191e-02 -1.89663132e-03 -1.35853933e-02 -4.04395955e-03\n",
            " -2.26868521e-02 -3.34946392e-03 -6.69917837e-03 -1.77069418e-02\n",
            "  4.02272260e-03 -3.71051067e-03 -4.08841018e-03 -8.47138558e-03\n",
            "  8.59416090e-03 -2.47625504e-02  1.47946505e-02  6.31314144e-03\n",
            "  7.41556752e-03  2.64555626e-02  1.57132633e-02  5.43127442e-03\n",
            " -1.42553477e-02  1.49903130e-02 -9.99411754e-03  1.27351121e-03\n",
            " -2.00902484e-02 -2.30467282e-02 -6.73067407e-04 -2.50840671e-02\n",
            "  5.30781271e-03  5.35172317e-03 -1.38330013e-02 -8.62203725e-03\n",
            "  5.91220241e-03 -9.36629809e-03  2.49168146e-02  9.49247368e-03\n",
            "  3.91714275e-03 -3.26828770e-02  2.31743092e-03 -3.14759910e-02\n",
            "  4.09560651e-02  3.11504118e-02  4.26668022e-03 -1.93064343e-02\n",
            "  2.96703237e-03  1.56500377e-02 -8.41652974e-03  2.26810463e-02\n",
            " -7.96514936e-03  3.35233775e-03 -3.35048256e-03 -2.55252961e-02\n",
            " -3.20236124e-02  7.49851763e-03 -2.40633357e-02  1.88961606e-02\n",
            " -2.27888562e-02  6.94493856e-03 -2.94721778e-02 -1.19394213e-02\n",
            " -3.05972202e-03  3.32077063e-04 -3.53663601e-03  3.41702327e-02\n",
            " -1.25093702e-02  1.95812508e-02 -4.07624233e-04  7.69273052e-03\n",
            " -2.11794600e-02  4.40781228e-02  2.80806824e-04 -1.51173947e-02\n",
            " -4.16141301e-02 -2.70501934e-02  1.04836887e-03 -3.54861841e-02\n",
            "  3.08933817e-02 -1.13309249e-02 -1.51189286e-02 -1.63109340e-02\n",
            "  1.49877192e-02 -1.33303534e-02  1.05245542e-02  1.09048104e-02\n",
            " -3.46808508e-03 -6.92403549e-03 -1.50944125e-02  2.52172109e-02\n",
            " -8.78196675e-04 -1.16015470e-03  2.44740513e-03 -1.71886962e-02\n",
            "  1.39940474e-02 -3.01014795e-03 -2.28596129e-03  2.84033408e-03\n",
            "  1.13767004e-02 -2.60093547e-02 -2.71433108e-02 -1.33114937e-03\n",
            "  1.64142940e-02  2.54202262e-03 -1.19430982e-02 -2.41898205e-02\n",
            " -1.00025721e-02 -3.28637287e-02 -1.14300996e-02 -5.72375068e-03\n",
            " -1.30048990e-02 -5.68614760e-03 -2.20316034e-02  2.59782076e-02\n",
            " -8.75111669e-03 -1.74961891e-02 -1.15337977e-02 -4.37902398e-02\n",
            " -3.42801102e-02  1.53001640e-02 -4.43357509e-03 -3.14349532e-02\n",
            " -2.62603600e-04  1.63769275e-02  1.59146672e-03  9.15230997e-03\n",
            "  1.83264408e-02 -1.59710720e-02 -2.63548438e-02  8.92740861e-03\n",
            "  7.36830989e-03  7.43357325e-03 -6.01058069e-04 -3.32707679e-03\n",
            " -2.64900876e-03  4.73456737e-03  1.69311520e-02 -2.10924707e-02\n",
            " -9.98355728e-03 -2.73906463e-03 -1.94152968e-03  4.72303387e-03\n",
            " -8.46970547e-03 -3.48933507e-04 -1.67809837e-02  2.45240080e-04\n",
            "  1.35598984e-02 -8.38198606e-03 -3.29015031e-02  1.48754939e-02\n",
            " -1.75703671e-02 -2.15857364e-02  8.52627959e-03  2.39862762e-02\n",
            " -2.23021284e-02  3.19918059e-03 -2.46722717e-02  1.30847963e-02\n",
            " -1.99888442e-02 -7.47156888e-03 -8.34135152e-03 -3.96983745e-03\n",
            "  1.42291957e-03 -1.42499118e-03 -6.57876069e-03 -2.51700776e-03\n",
            "  1.82759520e-02  2.51571052e-02  1.19315227e-02 -2.42483523e-02\n",
            " -3.93194426e-03  2.56534666e-02  2.87306914e-03  8.19804147e-04\n",
            "  9.87479463e-03 -1.74372196e-02 -2.32666289e-03  3.24777178e-02\n",
            " -9.51184891e-04  2.16959786e-04 -9.62951966e-03  2.38922657e-03\n",
            " -3.72816692e-03 -2.65904367e-02 -1.66354999e-02  1.78435221e-02\n",
            "  1.26571404e-02 -6.86924299e-03 -1.81326661e-02 -9.23372433e-03\n",
            " -9.00218170e-03  1.08578720e-03 -4.71630506e-02 -5.03222160e-02\n",
            " -4.12147958e-04  1.55825410e-02  8.02013837e-03  1.93321053e-02\n",
            " -4.56727901e-03 -1.54314684e-02 -7.97434151e-03 -2.10329425e-02\n",
            "  1.51790855e-02  3.70153901e-03 -7.22180773e-03 -1.00870794e-02\n",
            " -4.46812669e-03  7.23501388e-03  5.79578895e-03 -1.28512206e-02\n",
            " -4.43301303e-03  1.11253159e-02 -1.76201742e-02  1.81403030e-02\n",
            " -2.48397538e-03  6.53154496e-03 -2.24017967e-02  1.72106717e-02\n",
            " -1.84639394e-02 -5.92473336e-03  2.92306729e-02 -1.41911851e-02\n",
            "  1.86662115e-02  7.87336193e-03  1.63257234e-02  2.99712969e-03\n",
            " -1.26957463e-03 -1.59680527e-02  1.01977680e-02  1.69495791e-02\n",
            "  8.05753469e-03 -1.18765542e-02  6.60432503e-03  1.20032579e-02\n",
            "  1.33073349e-02 -1.14926267e-02 -1.10956589e-02  1.21817458e-02\n",
            " -1.07841389e-02  6.23741699e-03  2.53977720e-02  2.14331485e-02\n",
            "  4.36918903e-03 -6.73011178e-03 -6.81371614e-03 -1.26480740e-02\n",
            "  1.43387243e-02  2.78697852e-02 -2.23695673e-02  3.94665971e-02\n",
            " -7.85058551e-03  1.67767357e-04 -2.39766780e-02  2.68799462e-03\n",
            "  1.97127126e-02  1.83567461e-02  1.71846151e-02 -5.99628268e-03\n",
            " -4.21776343e-03  1.29469009e-02 -5.30095771e-03 -2.30679126e-03\n",
            "  1.05183795e-02 -1.81970056e-02 -5.30204270e-03  1.69383772e-02\n",
            "  1.08667919e-02 -3.08070816e-02  1.62642803e-02 -7.36691197e-03\n",
            "  1.12322271e-02  2.48284284e-02 -7.54532218e-03  1.91926230e-02\n",
            "  3.30587034e-03 -1.14347728e-03 -5.67451073e-03 -1.77233631e-03\n",
            " -5.33277029e-03 -8.57570395e-03  9.31160524e-03  2.47969292e-03\n",
            "  4.69090883e-03  2.73446925e-02  3.68605298e-03  8.43000226e-03\n",
            "  2.63430364e-02 -1.57106109e-02 -2.89585465e-03 -5.79892378e-03\n",
            " -2.19789823e-03 -1.21082540e-03 -1.97248161e-02 -4.44676774e-03\n",
            "  2.75000688e-02 -1.49147185e-02  2.74630589e-03 -7.47252489e-03\n",
            " -2.06379630e-02 -2.12786291e-02  9.01769754e-03  1.65569112e-02\n",
            "  2.04934441e-02  1.57253705e-02  1.50030972e-02  1.74466651e-02\n",
            " -2.35300697e-02  3.16070998e-03 -3.51460055e-02 -2.07555126e-02\n",
            " -1.26545373e-02  2.07587262e-03 -1.06951259e-02  2.75136926e-03\n",
            "  2.34522996e-03  8.63684807e-03  7.27548683e-03 -3.11507527e-02\n",
            " -2.24164650e-02  7.49077788e-03 -3.63411382e-02  9.25735570e-03\n",
            "  2.35186759e-02  1.37032177e-02  8.61264952e-03  1.05775958e-02\n",
            " -2.25416049e-02 -2.19591111e-02  7.90636055e-03 -1.40543859e-02\n",
            "  2.15484481e-03 -2.74050329e-02  3.54095595e-03  4.47373744e-03\n",
            " -2.96877380e-02  1.31015135e-02 -1.03962012e-02  2.68604513e-02\n",
            "  6.90451637e-03  1.24832522e-02 -1.62419416e-02 -1.20341508e-02\n",
            "  1.94401573e-02 -1.27709433e-02  1.20640267e-02 -1.78201012e-02\n",
            " -3.04398849e-03 -1.04523208e-02  6.76477049e-03 -5.23241796e-03\n",
            " -3.52720777e-03  2.61183567e-02  1.59176551e-02 -3.57541330e-02\n",
            "  1.76211335e-02  4.69859177e-03 -1.50583650e-03 -3.81165859e-03\n",
            " -1.50558380e-02  1.33306030e-02  2.12189592e-02  1.44062163e-02\n",
            "  3.19151282e-02 -8.22225772e-03 -2.43970342e-02 -5.53287659e-03\n",
            " -8.11152812e-03  1.49437636e-02 -3.37052858e-03 -1.27947805e-02\n",
            " -7.92884082e-03 -1.41807422e-02  3.95482779e-03 -1.48281902e-02\n",
            " -1.18416231e-02  1.62876528e-02 -1.57796480e-02 -1.57990381e-02\n",
            " -8.55760556e-03 -2.67932359e-02 -3.29221189e-02  8.70270655e-03\n",
            "  1.23322364e-02  5.62936440e-03  2.48095393e-02 -2.66589522e-02\n",
            " -1.65682938e-03  3.54981283e-03  2.67405603e-02  1.80946682e-02\n",
            "  6.32874807e-03 -2.45376471e-02  4.79351124e-03 -2.57603582e-02\n",
            "  1.97761431e-02 -1.29267890e-02  8.13836046e-03 -4.35012393e-03\n",
            " -3.92828370e-03 -1.24585154e-02  2.40262337e-02  1.53035522e-02\n",
            "  9.09479335e-03 -8.62512924e-03  2.47488860e-02  1.34013966e-02\n",
            "  3.00412830e-02  2.51423363e-02 -3.32345031e-02 -3.86758894e-03\n",
            "  8.94953590e-03 -2.02146340e-02  1.31501080e-02 -1.52401580e-02\n",
            "  3.06165731e-03 -2.16235481e-02 -1.95699446e-02 -3.29561755e-02\n",
            "  1.93286012e-03 -2.03238502e-02 -3.49172065e-03  1.78647991e-02\n",
            " -1.82586275e-02  6.32628507e-04 -6.36771205e-04  1.19167892e-02\n",
            "  2.49542817e-02  2.18252325e-03 -9.07785166e-03 -1.64851658e-02\n",
            "  9.19662416e-03 -8.48111324e-03  1.83094945e-02 -1.41107887e-02\n",
            " -3.21578421e-02  2.09761178e-03 -1.18726762e-02  2.97245625e-02\n",
            "  1.51866544e-02 -1.69236977e-02 -1.58049725e-02  6.71902765e-03\n",
            " -1.23970462e-02 -8.96789134e-03  7.65945483e-03 -1.44239198e-02\n",
            " -3.29977763e-03 -1.56138334e-02 -2.27893684e-02  2.15289779e-02\n",
            "  1.06800031e-02 -1.93735864e-02  3.60381848e-04  1.48387700e-02\n",
            " -3.43245454e-03  2.23273225e-02  1.33942813e-02 -2.67367158e-02\n",
            " -2.20456207e-03  1.01484302e-02 -7.03306752e-04  1.20268445e-02\n",
            "  9.16392426e-04  1.32511947e-02 -1.18756872e-02  1.80803798e-02\n",
            "  4.06286716e-02 -1.41962722e-03 -1.62280183e-02  1.97083168e-02\n",
            "  1.02007352e-02 -3.45117901e-03 -4.84661758e-03 -1.13915587e-02\n",
            "  1.64527707e-02  3.90379457e-03 -2.76981890e-02  4.10178676e-03\n",
            " -9.53011960e-03  1.37413677e-03  3.29271108e-02 -1.52619311e-03\n",
            " -1.11434404e-02  5.99141559e-03  6.26673410e-03 -2.42681801e-02\n",
            "  5.40636890e-02 -2.31021624e-02 -4.40809503e-03  3.40554193e-02\n",
            " -2.28370465e-02 -2.77883420e-03 -3.05340830e-02  9.78035759e-03\n",
            " -2.51283054e-03  4.81152348e-02 -1.18911928e-02 -4.32580896e-03\n",
            " -2.13699369e-03  1.53517853e-02  5.10804402e-03 -2.01489646e-02\n",
            "  2.04478111e-02 -2.23714318e-02 -5.47415111e-03  5.37062157e-03\n",
            " -2.01698416e-03 -1.46633610e-02  2.70452327e-03  5.75482845e-03\n",
            "  1.98200103e-02 -1.06508266e-02 -2.96117701e-02  8.35182052e-03\n",
            "  2.73156958e-03 -1.97022595e-02  4.81862500e-02  3.58079886e-03\n",
            " -1.76742021e-02  2.42660455e-02 -2.09346414e-02  1.87930707e-02\n",
            " -1.13638118e-04  2.10816227e-03  9.79073439e-03 -1.74039565e-02\n",
            " -1.20884097e-05  1.02152210e-02 -5.34805236e-03  2.12490503e-02\n",
            "  7.90356286e-03  1.67110970e-03  1.27846180e-02 -1.03238188e-02\n",
            " -9.65415593e-03  5.09615708e-03  9.89534892e-03 -4.97222599e-03\n",
            " -1.35815199e-02  4.62181773e-03 -2.24343874e-02 -2.20669084e-03\n",
            "  7.11283996e-04  2.39069331e-02 -3.67241316e-02 -7.86028244e-03\n",
            "  1.97332930e-02 -8.21969297e-04 -1.23657389e-02  6.04392635e-03\n",
            "  2.28724107e-02  1.71118770e-02 -2.11902522e-02 -1.14598405e-02\n",
            " -2.23288356e-04  2.13838685e-02  1.55869828e-04  8.86189472e-03\n",
            "  1.73674170e-02 -2.75064260e-02  3.57709825e-02  1.14035159e-02\n",
            "  5.38308080e-03 -9.42861103e-03  4.74738237e-03 -1.14275264e-02\n",
            " -4.78644483e-03 -5.69046801e-03 -1.61290336e-02  1.60614587e-02\n",
            "  8.48286599e-03  3.41410413e-02  8.15638714e-03  1.96357118e-03\n",
            "  1.29172932e-02  2.39294060e-02 -1.40355853e-02 -7.11820880e-03\n",
            " -3.46067995e-02 -3.61577049e-02 -6.37815241e-03  2.33399449e-03\n",
            "  4.99859510e-04  7.53314560e-03 -3.28867361e-02 -1.44000184e-02\n",
            " -3.22396085e-02  5.25713898e-03 -1.71729270e-02  3.89206631e-04\n",
            " -1.58988442e-02  1.62005965e-02 -2.78142542e-02 -1.24762431e-02\n",
            " -9.31040756e-03 -4.48674802e-03  4.74087661e-03 -3.22914653e-04\n",
            " -3.06347036e-03  9.84595809e-03 -2.77036484e-02  1.75896902e-02\n",
            "  1.02267247e-02  3.19605507e-02  4.83801076e-03  3.54361571e-02\n",
            " -3.05571370e-02  4.21029981e-03  2.15807203e-02  2.95000728e-02\n",
            " -1.46013591e-03  1.63031113e-03  9.05569084e-03 -2.93188877e-02\n",
            "  3.54463793e-03  1.15286782e-02 -1.19879236e-03  8.88841785e-03\n",
            " -6.23305421e-03 -3.53974360e-03 -1.40607599e-02  8.99077021e-03\n",
            "  1.82337221e-02  1.50403846e-02 -2.19072811e-02 -4.63306392e-03\n",
            "  4.67772176e-03 -6.80174399e-03  1.95991341e-02 -1.20552126e-02\n",
            "  3.98978442e-02  4.43580188e-03 -6.68258406e-04  2.51256768e-02\n",
            "  5.52743440e-03  4.30435687e-02 -1.28457416e-02  9.33219306e-03\n",
            " -2.28882767e-02 -6.12931140e-03 -4.72240662e-03 -1.87481456e-02\n",
            "  2.90883072e-02  5.51586854e-04  5.98951150e-03  2.12038794e-04\n",
            " -2.66817715e-02  1.18093137e-02 -5.58378641e-03 -2.87872609e-02\n",
            "  4.77392562e-02  1.42390979e-02  2.00923923e-02 -7.35621061e-03\n",
            "  2.68213935e-02  3.61217512e-03 -2.10020486e-02  2.66483165e-02\n",
            " -8.46839044e-03 -1.96761694e-02 -5.72162168e-03  1.96504686e-02\n",
            "  1.35205956e-02  7.48518342e-03  2.42535258e-04 -1.88108289e-03\n",
            " -2.09833588e-02 -1.26889329e-02  3.95568833e-02  8.16306937e-03\n",
            "  6.17502956e-03  5.55442739e-03  8.65833648e-03 -4.27027605e-03\n",
            "  1.26492726e-02  2.19252892e-03  1.16702188e-02 -4.99996077e-03\n",
            "  1.50515167e-02  2.80890167e-02  1.22436304e-02  2.14394741e-03\n",
            " -1.97410374e-03  7.40602892e-03 -4.11170302e-03  2.81424858e-02\n",
            " -1.31509481e-02 -3.55998054e-03  1.12362225e-02 -1.18316691e-02\n",
            "  1.50253568e-02 -2.00725682e-02  5.53910970e-04  1.19521711e-02\n",
            " -1.57285556e-02 -4.71324939e-03  2.02265345e-02 -1.92956012e-02\n",
            " -4.82574245e-03  2.44383328e-02  9.13385302e-03 -1.61433197e-03\n",
            "  2.93301679e-02  9.41964798e-04 -2.28719600e-02 -1.10789724e-02\n",
            " -2.01671831e-02  1.33789759e-02 -3.06857633e-03 -5.41144563e-03\n",
            "  1.30686181e-04 -8.39624438e-04  1.77627429e-03  3.87565373e-03\n",
            " -2.25047953e-02  7.81886559e-03 -1.71808805e-02 -1.09356046e-02\n",
            " -2.10266374e-02  1.44083155e-02 -7.10903085e-04 -1.32136000e-02\n",
            " -2.37256847e-02 -2.07390985e-03  1.65370274e-02  1.91078149e-02\n",
            " -6.32681837e-03 -4.31305869e-03  1.00161750e-02  9.13793338e-05\n",
            "  1.45342825e-02 -1.52355134e-02 -1.82658862e-02 -5.64384274e-02\n",
            "  1.36541333e-02  7.41109019e-03 -1.85949691e-02 -7.53732806e-04\n",
            " -6.17241347e-03 -5.56729641e-03  8.21191352e-03 -1.18530216e-02\n",
            "  3.58729921e-02 -2.59558298e-03  1.68076809e-02  3.45744826e-02\n",
            "  2.67328024e-02 -1.26864796e-03  3.11014382e-03  1.05024483e-02\n",
            "  1.23864291e-02  1.48843052e-02 -2.50524376e-02 -5.75247686e-03\n",
            " -2.08117552e-02 -9.19425394e-03 -7.32812798e-03 -3.18328093e-04\n",
            "  2.61761807e-02 -3.13399360e-03  9.10258573e-03  1.04701025e-02\n",
            " -2.26859469e-02  1.29525485e-02 -1.47348521e-02 -1.18438406e-02\n",
            "  2.74164631e-04  7.60663766e-03 -1.35675222e-02  3.26789683e-03\n",
            " -3.54315378e-02  5.19553432e-03  5.22248866e-03 -4.62064659e-03\n",
            "  3.28316502e-02 -2.98651168e-03 -1.24416891e-02  9.85025242e-03\n",
            " -4.73268107e-02 -1.22340564e-02  2.09988896e-02 -1.84492301e-03\n",
            "  1.46257682e-02 -2.51478199e-02  6.57066097e-03 -3.37610617e-02\n",
            "  9.87942680e-04 -1.83643959e-02 -2.11779727e-03 -3.50197256e-02\n",
            "  1.00877266e-02 -2.30111968e-04 -1.26911579e-02  1.99013706e-02\n",
            " -2.48423666e-02 -1.27823474e-02 -4.06095199e-02 -3.34291207e-03\n",
            " -2.20519304e-02 -2.43577566e-02  4.04080339e-02 -9.52277519e-03\n",
            "  2.41071964e-03  2.30207909e-02  8.06544628e-03  2.99054384e-03\n",
            "  6.77943090e-03 -3.30307931e-02 -9.29580070e-03  3.25891259e-03\n",
            " -6.14743400e-03 -1.74718874e-03 -1.31200776e-02 -2.23606322e-02\n",
            " -7.37993047e-03 -3.06593515e-02  7.16049224e-03  3.41479778e-02\n",
            " -8.97507928e-03 -2.97714919e-02  8.80649127e-03 -4.30052727e-03\n",
            " -2.68446445e-03 -1.27022685e-02 -3.54275666e-02 -1.03379320e-02\n",
            " -4.45052423e-03  3.13108899e-02 -3.24845198e-03 -5.57946786e-03\n",
            "  1.60582028e-02 -3.25387157e-02  2.39039399e-02 -1.18530057e-02\n",
            "  1.73862651e-03  5.11775240e-02 -4.22291271e-03  1.03553040e-02\n",
            " -2.30833068e-02  9.81882680e-03  1.13982139e-02 -6.43765146e-04\n",
            "  1.20337424e-03  8.28178786e-03 -4.84281965e-02  1.71263777e-02\n",
            "  4.64178243e-04 -2.37118006e-02 -3.76097150e-02  9.69659886e-04\n",
            " -1.20496731e-02 -6.00575190e-03  1.43798413e-02 -2.66060289e-02\n",
            " -2.45185336e-03 -9.40213352e-03  1.89307965e-02 -1.36672705e-02\n",
            " -3.69262369e-03  1.11648533e-02  1.19858701e-02  2.03841322e-04\n",
            "  2.45721103e-03  1.11172367e-02 -8.70074704e-03  2.67947558e-02\n",
            " -1.42313857e-02  1.99502893e-02  5.53091476e-03  1.16515886e-02\n",
            "  6.76885108e-03  8.68920796e-03 -2.37472355e-03  1.71550515e-03\n",
            "  2.28765290e-02 -5.66186197e-03  1.19803390e-02  1.09632371e-03\n",
            "  3.74898240e-02 -1.96879413e-02  2.11507641e-02  3.00224479e-02\n",
            " -9.60247777e-03  3.09646837e-02  3.74637805e-02  9.52236820e-04\n",
            "  1.31064113e-02  1.79719143e-02 -2.87136286e-02 -2.01235786e-02\n",
            " -1.29558306e-04  1.06631301e-03 -1.41561013e-02  2.65303310e-02\n",
            "  6.37014443e-03  1.10261738e-02 -1.35986106e-02  6.78608567e-03\n",
            " -2.04536808e-03  1.40687991e-02 -1.66527051e-02 -6.28723670e-03]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuWm23heXV7w",
        "outputId": "4b051e22-aa89-4797-b281-50ce2c71165f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Store the vectors for train data in following file\n",
        "word2vec_filename_train = '/content/sample_data' + 'train_review_word2vec.csv'\n",
        "with open(word2vec_filename_train, 'w+') as word2vec_file:\n",
        "    for index, row in x_train.iterrows():\n",
        "      # print( index )\n",
        "        model_vector = (np.mean([sg_w2v_model[token] for token in row['stemmed_tokens']], axis=0)).tolist()\n",
        "        if index == 0:\n",
        "            header = \",\".join(str(ele) for ele in range(size))\n",
        "            word2vec_file.write(header)\n",
        "            word2vec_file.write(\"\\n\")\n",
        "        # Check if the line exists else it is vector of zeros\n",
        "        if type(model_vector) is list:  \n",
        "            line1 = \",\".join( [str(vector_element) for vector_element in model_vector] )\n",
        "        else:\n",
        "            line1 = \",\".join([str(0) for i in range(size)])\n",
        "        word2vec_file.write(line1)\n",
        "        word2vec_file.write('\\n')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQZySMuo3LeY",
        "outputId": "1c1e3ffa-a64f-46d6-c635-86cb7c69889b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Store the vectors for train data in following file\n",
        "word2vec_filename_test = '/content/sample_data' + 'test_review_word2vec.csv'\n",
        "with open(word2vec_filename_test, 'w+') as word2vec_file1:\n",
        "    for index, row in x_test.iterrows():\n",
        "      # print( index )\n",
        "        model_vector = (np.mean([sg_w2v_model[token] for token in row['stemmed_tokens']], axis=0)).tolist()\n",
        "        if index == 0:\n",
        "            header = \",\".join(str(ele) for ele in range(size))\n",
        "            word2vec_file.write(header)\n",
        "            word2vec_file.write(\"\\n\")\n",
        "        # Check if the line exists else it is vector of zeros\n",
        "        if type(model_vector) is list:  \n",
        "            line1 = \",\".join( [str(vector_element) for vector_element in model_vector] )\n",
        "        else:\n",
        "            line1 = \",\".join([str(0) for i in range(size)])\n",
        "        word2vec_file1.write(line1)\n",
        "        word2vec_file1.write('\\n')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZYMQpOfkRoX",
        "outputId": "22042c3b-4a0f-4f1c-d185-9ac94f3a741d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "\n",
        "df[436:438]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Label</th>\n",
              "      <th>tokenized_text</th>\n",
              "      <th>stemmed_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>436</th>\n",
              "      <td>You can ask me to call you a male, but you'll...</td>\n",
              "      <td>0</td>\n",
              "      <td>[you, can, ask, me, to, call, you, male, but, ...</td>\n",
              "      <td>[you, can, ask, me, to, call, you, male, but, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>Yo, if I’m going to hell for being homosexual...</td>\n",
              "      <td>0</td>\n",
              "      <td>[yo, if, going, to, hell, for, being, homosexu...</td>\n",
              "      <td>[yo, if, go, to, hell, for, be, homosexu, why,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Sentiment  ...                                     stemmed_tokens\n",
              "436   You can ask me to call you a male, but you'll...  ...  [you, can, ask, me, to, call, you, male, but, ...\n",
              "437   Yo, if I’m going to hell for being homosexual...  ...  [yo, if, go, to, hell, for, be, homosexu, why,...\n",
              "\n",
              "[2 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsWanzbI-b4X",
        "outputId": "1dc8b98a-fdab-4416-bdcd-2603be01aa69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# y_train.head(),\n",
        "\n",
        "y_train.shape,x_train.shape,x_test.shape,y_test.shape\n",
        "# y_test.head()\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((798, 1), (798, 1), (200, 1), (200,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S8-cWKUX5Qc",
        "outputId": "289fb41d-1db0-47f2-e7fa-f051927eb5bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import time\n",
        "#Import the DecisionTreeeClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "# Load from the filename\n",
        "word2vec_df_train = pd.read_csv(word2vec_filename_train)\n",
        "word2vec_df_train.shape\n",
        "# Initialize the model\n",
        "# clf_decision_word2vec = DecisionTreeClassifier()\n",
        "\n",
        "clf1 = DecisionTreeClassifier()\n",
        "clf2 = LogisticRegression()\n",
        "clf3 = RandomForestClassifier()\n",
        "clf4 = MLPClassifier(max_iter=2000, shuffle=True, hidden_layer_sizes=(16, 8) , activation='relu', solver='adam')\n",
        "clf5 = KNeighborsClassifier()\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "# Fit the model\n",
        "clf1.fit(word2vec_df_train, y_train['Label'])\n",
        "clf2.fit(word2vec_df_train, y_train['Label'])\n",
        "clf3.fit(word2vec_df_train, y_train['Label'])\n",
        "clf4.fit(word2vec_df_train, y_train['Label'])\n",
        "clf5.fit(word2vec_df_train, y_train['Label'])\n",
        "classifiers = [(\"DecisionTreeclassifier\",clf1),(\"LogisticRegression\",clf2),(\"RandomForest\",clf3),(\"MLP\",clf4),(\"KNN\",clf5)]\n",
        "print(\"Time taken to fit the model with word2vec vectors: \" + str(time.time() - start_time))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time taken to fit the model with word2vec vectors: 2.3270397186279297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lg3m8ngbT9PG",
        "outputId": "05251eda-ca1e-4f3c-d6a1-ab7da97aebf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lua1MIK-80M8",
        "outputId": "0bd4e5b0-869f-40ef-ae67-79b1e02f6e11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for clf in classifiers:\n",
        "  word2vec_df_test = pd.read_csv(word2vec_filename_test, header=None)\n",
        "  test_preds = clf[1].predict(word2vec_df_test.to_numpy())\n",
        "  print(clf[0] + \" \"+ str(1 -  sum(abs(y_test - test_preds))/y_test.shape[0]))\n",
        "  # print(classification_report(y_test, test_preds))\n",
        "  # test_preds.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DecisionTreeclassifier 0.47\n",
            "LogisticRegression 0.5449999999999999\n",
            "RandomForest 0.5449999999999999\n",
            "MLP 0.5449999999999999\n",
            "KNN 0.51\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxNDHv3M-Uun",
        "outputId": "4eefd643-a500-4cd2-eee2-aa5c2303d795",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install tabulate\n",
        "from tabulate import tabulate\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "# Note: to install the `tabulate` package, \n",
        "#     go to a shell terminal and run the command\n",
        "#     `conda install tabulate`\n",
        "# import tabulate\n",
        "table = [[\"Model\",\"Data Set\",\"VectorRepresentation\",\"Validation Accuracy(%)\"],\n",
        "         [\"Decision Tree Classifier\",\"Ethos Binary Dataset\", \"Word2vec\",\"47.0\"],\n",
        "         [\"Logistic Regression\",\"Ethos Binary Dataset\", \"Word2vec\",\"54.49\"],\n",
        "         [\"Random Forest\",\"Ethos Binary Dataset\", \"Word2vec\",\"54.49\"],\n",
        "         [\"MLP\",\"Ethos Binary Dataset\", \"Word2vec\",\"54.49\"],\n",
        "         [\"KNN\",\"Ethos Binary Dataset\", \"Word2vec\",\"51.0\", ]]\n",
        "print(tabulate(table))\n",
        "# display(HTML(tabulate.tabulate(table, tablefmt='html')))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (0.8.7)\n",
            "------------------------  --------------------  --------------------  ----------------------\n",
            "Model                     Data Set              VectorRepresentation  Validation Accuracy(%)\n",
            "Decision Tree Classifier  Ethos Binary Dataset  Word2vec              47.0\n",
            "Logistic Regression       Ethos Binary Dataset  Word2vec              54.49\n",
            "Random Forest             Ethos Binary Dataset  Word2vec              54.49\n",
            "MLP                       Ethos Binary Dataset  Word2vec              54.49\n",
            "KNN                       Ethos Binary Dataset  Word2vec              51.0\n",
            "------------------------  --------------------  --------------------  ----------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOlnYy_Q3JJ4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFtgeeDmNToL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}